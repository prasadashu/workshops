{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/sachin-rastogi/workshops/blob/master/PyTorch/pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PyTorch Logo](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is PyTorch?  \n",
    "===============\n",
    "It’s a Python-based scientific computing package targeted at following sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- A deep learning research platform that provides maximum flexibility and speed\n",
    "- A Deep neural networks built on a tape-based autograd system\n",
    "\n",
    "\n",
    "At a granular level, PyTorch is a library that consists of the following components:\n",
    "\n",
    "| Component | <div style=\"text-align: justify\"> Description </div>|\n",
    "| ---- | --- |\n",
    "| [**torch**](https://pytorch.org/docs/stable/torch.html) | <div style=\"text-align: justify\"> A Tensor library like NumPy, with strong GPU support </div>|\n",
    "| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | <div style=\"text-align: justify\"> DataLoader and other utility functions for convenience </div>|\n",
    "| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | <div style=\"text-align: justify\"> A neural networks library deeply integrated with autograd designed for maximum flexibility </div>|\n",
    "| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | <div style=\"text-align: justify\"> A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch. PyTorch uses it to calculate gradients for training neural networks. Autograd, does all the work of backpropagation by calculating the gradients at each operation in the network which we can then use to update the network weights. </div>|\n",
    "| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | <div style=\"text-align: justify\"> A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n",
    "| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | <div style=\"text-align: justify\"> Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training </div>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GPU-Ready Tensor Library\n",
    "------------------------------------------\n",
    "If you use NumPy, then you have used Tensors (a.k.a. ndarray).\n",
    "\n",
    "![Tensor illustration](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/tensor_illustration.png)\n",
    "\n",
    "PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerates the\n",
    "computation by a huge amount.\n",
    "\n",
    "We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\n",
    "such as slicing, indexing, math operations, linear algebra, reductions.\n",
    "And they are fast!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic introduction to PyTorch  \n",
    "\n",
    "We'll cover tensors - the main data structure of PyTorch. \n",
    "- how to create tensors, \n",
    "- how to do simple operations, and \n",
    "- how tensors interact with NumPy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "**Tensors** a generalization of matrices. \n",
    "- A vector is a 1-dimensional tensor, \n",
    "- a matrix is a 2-dimensional tensor, \n",
    "- an array with three indices is a 3-dimensional tensor (RGB color images for example).  \n",
    "\n",
    "The basic data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.  \n",
    "\n",
    "*A neural network computations are just a bunch of linear algebra operations on tensors*.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors** are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:07.955358Z",
     "start_time": "2020-08-09T18:22:07.381849Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:07.959367Z",
     "start_time": "2020-08-09T18:22:07.956353Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor Parameters:**  \n",
    "\n",
    "- size..................... : Size of the tensor.   \n",
    "- out=None................. : Providing an output tensor as argument.\n",
    "- dtype=None............... : A torch.dtype is an object that represents the data type of a torch.Tensor.  \n",
    "- layout=torch.strided..... : A torch.layout is an object that represents the memory layout of a torch.Tensor. Currently, we support torch.strided (dense Tensors) and have beta support for torch.sparse_coo (sparse COO Tensors).   \n",
    "- device=None.............. : A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.  \n",
    "- requires_grad=False...... :  If autograd should record operations on the returned tensor. I.e. It should be True if gradients need to be computed for this Tensor, False otherwise.  \n",
    "\n",
    "More read : https://pytorch.org/docs/stable/tensor_attributes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor` is an alias for the default tensor type (`torch.FloatTensor`).\n",
    "\n",
    "A tensor can be constructed from a Python list or sequence using the `torch.tensor()` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:07.970275Z",
     "start_time": "2020-08-09T18:22:07.960313Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor([1, 2, 3], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:07.978970Z",
     "start_time": "2020-08-09T18:22:07.971149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a 5x3 matrix, uninitialized: When an uninitialized matrix is created, whatever values were in \n",
    "# the allocated memory at the time will appear as the initial values.\n",
    "\n",
    "x = torch.empty(5, 3)\n",
    "print(x, x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:07.987704Z",
     "start_time": "2020-08-09T18:22:07.979796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a randomly initialized matrix:\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:07.996740Z",
     "start_time": "2020-08-09T18:22:07.988475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1\n",
    "\n",
    "xn = torch.randn(50,10, dtype=torch.float32)\n",
    "print(torch.std_mean(xn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.005264Z",
     "start_time": "2020-08-09T18:22:07.997506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a matrix filled zeros and of dtype long:\n",
    "\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.014100Z",
     "start_time": "2020-08-09T18:22:08.006484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a matrix filled ones and of dtype long:\n",
    "\n",
    "x = torch.ones(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.022878Z",
     "start_time": "2020-08-09T18:22:08.015071Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a tensor directly from data:\n",
    "\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.032237Z",
     "start_time": "2020-08-09T18:22:08.023644Z"
    }
   },
   "outputs": [],
   "source": [
    "# - Create a tensor based on an existing tensor. \n",
    "# - These methods will reuse properties of the input tensor,\n",
    "# e.g. dtype, unless new values are provided by user\n",
    "\n",
    "x = x.new_ones(5, 3, dtype=torch.int16)      # Changing an existing tensor, new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "y = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.040366Z",
     "start_time": "2020-08-09T18:22:08.032991Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get its size: Use size or shape\n",
    "\n",
    "print(x.size(), y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.049550Z",
     "start_time": "2020-08-09T18:22:08.041125Z"
    }
   },
   "outputs": [],
   "source": [
    "# If we have an one element tensor, we can use .item() to get the value as a Python number\n",
    "\n",
    "x1 = torch.randn(1)\n",
    "print(x1, type(x1))\n",
    "\n",
    "print(x1.item(), type(x1.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.058164Z",
     "start_time": "2020-08-09T18:22:08.050315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Operations\n",
    "# There are multiple syntaxes for operations. \n",
    "# Addition: syntax 1\n",
    "\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.066961Z",
     "start_time": "2020-08-09T18:22:08.058942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Addition: syntax 2\n",
    "\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.076345Z",
     "start_time": "2020-08-09T18:22:08.067725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Addition: providing an output tensor as argument\n",
    "\n",
    "result = torch.empty(5, 3) #define output tensor\n",
    "\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.085052Z",
     "start_time": "2020-08-09T18:22:08.077114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Addition: in-place\n",
    "# adds x to y\n",
    "# Any operation that mutates a tensor in-place is post-fixed with an _. \n",
    "# For example: x.copy_(y), x.t_(), will change x.\n",
    "\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.121369Z",
     "start_time": "2020-08-09T18:22:08.085839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subtraction\n",
    "a = np.array(5)\n",
    "b = torch.tensor([2, 4, 6, 8, 10])\n",
    "\n",
    "print(torch.is_tensor(a), \" \", torch.is_tensor(b))\n",
    "\n",
    "# Subtraction\n",
    "print(\"Subtraction: {}\\n\".format(b.sub(b)))\n",
    "\n",
    "# Element wise multiplication\n",
    "print(\"Element wise multiplication: {}\\n\".format(torch.mul(b,b)))\n",
    "\n",
    "# Element wise division\n",
    "print(\"Element wise division: {}\\n\".format(torch.true_divide(b,b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:08.124655Z",
     "start_time": "2020-08-09T18:22:08.122140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "tensor = torch.Tensor([1,2,3,4,5])\n",
    "print(\"Mean: {}\".format(tensor.mean()))\n",
    "\n",
    "# Standart deviation (std)\n",
    "print(\"std: {}\".format(tensor.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.734851Z",
     "start_time": "2020-08-09T18:22:08.125449Z"
    }
   },
   "outputs": [],
   "source": [
    "# CUDA Tensors\n",
    "# Tensors can be moved onto any device using the .to method.\n",
    "\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(\"\\n\")\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!\n",
    "    \n",
    "    # Mix and Match\n",
    "#     y1 = torch.ones_like(x, device=device)\n",
    "#     y2 = torch.ones_like(x, device=\"cpu\")\n",
    "#     z1 = y1 + y2\n",
    "#     print(z1) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.738385Z",
     "start_time": "2020-08-09T18:22:10.735793Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.IS_FLOATING_POINT\n",
    "\n",
    "c = torch.randn(5,3)\n",
    "print(c, \"\\n\", torch.is_floating_point(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.761944Z",
     "start_time": "2020-08-09T18:22:10.739204Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.IS_NONZERO\n",
    "\n",
    "print(torch.is_nonzero(torch.tensor([0.])))\n",
    "print(\"\\n\")\n",
    "print(torch.is_nonzero(torch.tensor([1.5])))\n",
    "print(\"\\n\")\n",
    "print(torch.is_nonzero(torch.tensor([False])))\n",
    "print(\"\\n\")\n",
    "#torch.is_nonzero(torch.tensor([1, 3, 5])) # RuntimeError: bool value of Tensor with more than one value is ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.770062Z",
     "start_time": "2020-08-09T18:22:10.762741Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.ARANGE\n",
    "# torch.arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "\n",
    "print(torch.arange(5))\n",
    "print(torch.arange(1, 4))\n",
    "print(torch.arange(1, 2.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.777951Z",
     "start_time": "2020-08-09T18:22:10.772192Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.GET_NUM_THREADS\n",
    "# Returns the number of threads used for parallelizing CPU operations\n",
    "\n",
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.785572Z",
     "start_time": "2020-08-09T18:22:10.779204Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.SET_NUM_THREADS\n",
    "# Sets the number of threads used for intraop parallelism on CPU. \n",
    "# WARNING: To ensure that the correct number of threads is used, set_num_threads must be called before running \n",
    "# eager, JIT or autograd code.\n",
    "\n",
    "torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.795487Z",
     "start_time": "2020-08-09T18:22:10.786355Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.SIGMOID\n",
    "# Returns a new tensor with the sigmoid of the elements of input.\n",
    "display(Math(r'\\text{out}_{i} = \\frac{1}{1 + e^{-\\text{input}_{i}}}'))\n",
    "\n",
    "a = torch.randn(4)\n",
    "print(\"Input Tensor :\", a, \"\\n Output Tensor :\", torch.sigmoid(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.804377Z",
     "start_time": "2020-08-09T18:22:10.796229Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.ARGMAX\n",
    "# Returns the indices of the maximum value of all elements in the input tensor.\n",
    "# This is the second value returned by torch.max().\n",
    "\n",
    "a = torch.randn(4, 4)\n",
    "print(a, \"\\n\", torch.argmax(a), \"\\n\", a.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.812329Z",
     "start_time": "2020-08-09T18:22:10.805158Z"
    }
   },
   "outputs": [],
   "source": [
    "# dim=1 i.e. across rows, dim=0 i.e. across columns\n",
    "torch.argmax(a, dim=1) # Across rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.820447Z",
     "start_time": "2020-08-09T18:22:10.813062Z"
    }
   },
   "outputs": [],
   "source": [
    "z = torch.argmax(a, dim=1, keepdim=True) # Across rows\n",
    "print(z, \"\\n\", z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.828605Z",
     "start_time": "2020-08-09T18:22:10.821211Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.argmax(a, dim=0) # Across columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.836862Z",
     "start_time": "2020-08-09T18:22:10.829356Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.MAX\n",
    "# Returns the maximum value of all elements in the input tensor.\n",
    "\n",
    "print(a, \"\\n\", torch.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.845698Z",
     "start_time": "2020-08-09T18:22:10.837618Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.MAX \n",
    "# Across rows\n",
    "torch.max(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.853595Z",
     "start_time": "2020-08-09T18:22:10.846509Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.MANUAL_SEED\n",
    "# Sets the seed for generating random numbers. Returns a torch.Generator object.\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "print(torch.rand(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.862462Z",
     "start_time": "2020-08-09T18:22:10.854644Z"
    }
   },
   "outputs": [],
   "source": [
    "a[3,3], a[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy to Torch and back  \n",
    "PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use torch.from_numpy(). To convert a tensor to a Numpy array, use the .numpy() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.870920Z",
     "start_time": "2020-08-09T18:22:10.863243Z"
    }
   },
   "outputs": [],
   "source": [
    "# NumPy Bridge\n",
    "# The Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU),\n",
    "# and changing one will change the other.\n",
    "# Converting a Torch Tensor to a NumPy Array\n",
    "\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "\n",
    "print(a, \"\\n\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.879378Z",
     "start_time": "2020-08-09T18:22:10.871798Z"
    }
   },
   "outputs": [],
   "source": [
    "# See how the numpy array changed in value.\n",
    "\n",
    "a.add_(1) # Inplace addition\n",
    "\n",
    "print(a, \"\\n\\n\", b) #  The Torch Tensor and NumPy array will share their underlying memory locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.888478Z",
     "start_time": "2020-08-09T18:22:10.880257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Python list\n",
    "array = [[1,2,3],[4,5,6]]\n",
    "\n",
    "# numpy array\n",
    "first_array = np.array(array) # 2x3 array\n",
    "\n",
    "print(\"Array Type: {}\".format(type(first_array))) # type\n",
    "print(\"Array Shape: {}\".format(np.shape(first_array))) # shape\n",
    "print(type(first_array), \"\\n\", first_array)\n",
    "print(type(array), \"\\n\", array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.897309Z",
     "start_time": "2020-08-09T18:22:10.889269Z"
    }
   },
   "outputs": [],
   "source": [
    "# pytorch array\n",
    "tensor = torch.Tensor(array)\n",
    "\n",
    "print(\"Array Type: {}\".format(tensor.dtype)) # type\n",
    "print(\"Array Shape: {}\".format(tensor.shape)) # shape\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.905872Z",
     "start_time": "2020-08-09T18:22:10.898109Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy ones\n",
    "print(\"Numpy {}\\n\".format(np.ones((2,3))))\n",
    "\n",
    "# pytorch ones\n",
    "print(\"PyTorch {}\\n\".format(torch.ones((2,3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.914724Z",
     "start_time": "2020-08-09T18:22:10.906761Z"
    }
   },
   "outputs": [],
   "source": [
    "# numpy random\n",
    "print(\"Numpy {}\\n\".format(np.random.rand(2,3)))\n",
    "\n",
    "# pytorch random\n",
    "print(\"PyTorch {}\\n\".format(torch.rand(2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.924360Z",
     "start_time": "2020-08-09T18:22:10.915603Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can use standard NumPy-like indexing with all bells and whistles!\n",
    "\n",
    "print(y[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.933090Z",
     "start_time": "2020-08-09T18:22:10.925148Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting NumPy Array to Torch Tensor\n",
    "# See how changing the np array changed the Torch Tensor automatically\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "np.add(a, 1, out=a)\n",
    "\n",
    "print(a, \"\\n\\n\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.941380Z",
     "start_time": "2020-08-09T18:22:10.933903Z"
    }
   },
   "outputs": [],
   "source": [
    "# TORCH.IS_TENSOR\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "print(torch.is_tensor(a), \" \", torch.is_tensor(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.950832Z",
     "start_time": "2020-08-09T18:22:10.942220Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.959998Z",
     "start_time": "2020-08-09T18:22:10.951707Z"
    }
   },
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)\n",
    "print(b, \"\\n\", b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.968327Z",
     "start_time": "2020-08-09T18:22:10.960917Z"
    }
   },
   "outputs": [],
   "source": [
    "# The memory is shared between the Numpy array and Torch tensor, so if we change the values in-place of one object,\n",
    "# the other will change as well.\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.976583Z",
     "start_time": "2020-08-09T18:22:10.969243Z"
    }
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T08:23:32.898477Z",
     "start_time": "2020-08-09T08:23:32.896564Z"
    }
   },
   "source": [
    "### Flatten, Reshape, And Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.984926Z",
     "start_time": "2020-08-09T18:22:10.977476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resizing: If we want to resize/reshape tensor, you can use torch.view:\n",
    "# A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution.\n",
    "\n",
    "x = torch.randn(3, 4) # Random Normal variables which have properties like mean 0 and std as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch the size and shape of a tensor mean the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rank**, commonly used as the number of dimensions of the tensor. The `rank` of a tensor is equal to the length of the tensor's shape. We can deduce a couple of things from `rank`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:10.994908Z",
     "start_time": "2020-08-09T18:22:10.985697Z"
    }
   },
   "outputs": [],
   "source": [
    "x.shape, len(x.shape) # So Rank is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.004300Z",
     "start_time": "2020-08-09T18:22:10.995685Z"
    }
   },
   "outputs": [],
   "source": [
    "# deduce the number of elements contained within the tensor. The number of elements inside a tensor is equal to \n",
    "# the product of the shape's component values.\n",
    "torch.tensor(x.shape).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.012175Z",
     "start_time": "2020-08-09T18:22:11.005060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Although, In PyTorch, there is a dedicated function for this\n",
    "\n",
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of elements contained within a tensor is important for reshaping because the reshaping must account for the total number of elements present. Reshaping changes the tensor's shape but not the underlying data. Our tensor has 12 elements, so any reshaping must account for exactly 12 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.024167Z",
     "start_time": "2020-08-09T18:22:11.012925Z"
    }
   },
   "outputs": [],
   "source": [
    "# With Rank 2\n",
    "x.reshape([1,12]), x.reshape([2,6]), x.reshape([3,-1]), x.reshape([-1,3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.032357Z",
     "start_time": "2020-08-09T18:22:11.024962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Increase the rank to 3\n",
    "\n",
    "x.reshape(2,2,3), len(x.reshape(2,2,3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.040896Z",
     "start_time": "2020-08-09T18:22:11.033097Z"
    }
   },
   "outputs": [],
   "source": [
    "x # I.e. No change to actual x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has another function that we may see called view() that does the same thing as the reshape()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.049234Z",
     "start_time": "2020-08-09T18:22:11.041649Z"
    }
   },
   "outputs": [],
   "source": [
    "y = x.view(12)\n",
    "\n",
    "z = x.view(-1, 6)  # the size -1 is inferred from other dimensions\n",
    "\n",
    "print(x.size(), y.size(), z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.058245Z",
     "start_time": "2020-08-09T18:22:11.050026Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x, \"\\n\"*2, y, \"\\n\"*2, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing Shape By Squeezing And Unsqueezing:**  \n",
    "\n",
    "we can change the shape of our tensors is by `squeezing` and `unsqueezing` them.\n",
    "\n",
    "- Squeezing a tensor removes the dimensions or axes that have a length of one.\n",
    "- Unsqueezing a tensor adds a dimension with a length of one.  \n",
    "\n",
    "These functions allow us to expand or shrink the rank (number of dimensions) of our tensor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.067018Z",
     "start_time": "2020-08-09T18:22:11.059013Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(x.reshape([1,12]))\n",
    "print(x.reshape([1,12]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.075547Z",
     "start_time": "2020-08-09T18:22:11.067794Z"
    }
   },
   "outputs": [],
   "source": [
    "# squeeze\n",
    "print(x.reshape([1,12]).squeeze(), \"\\n\", \n",
    "      x.reshape([1,12]).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.083913Z",
     "start_time": "2020-08-09T18:22:11.076354Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x.reshape([1, 3, 4]).squeeze(), \"\\n\", \n",
    "      x.reshape([1, 3, 4]).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.092159Z",
     "start_time": "2020-08-09T18:22:11.084702Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x.reshape([3, 4, 1]).squeeze(), \"\\n\", \n",
    "      x.reshape([3, 4, 1]).squeeze().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.100910Z",
     "start_time": "2020-08-09T18:22:11.092944Z"
    }
   },
   "outputs": [],
   "source": [
    "# unsqueeze\n",
    "print(x.reshape([3, 4, 1]).squeeze().unsqueeze(dim=0), \"\\n\", \n",
    "      x.reshape([3, 4, 1]).squeeze().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.109122Z",
     "start_time": "2020-08-09T18:22:11.101710Z"
    }
   },
   "outputs": [],
   "source": [
    "# unsqueeze\n",
    "print(x.reshape([3, 4, 1]).squeeze().unsqueeze(dim=1), \"\\n\", \n",
    "      x.reshape([3, 4, 1]).squeeze().unsqueeze(dim=1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at a common use case for squeezing a tensor by building a flatten function.\n",
    "\n",
    "**Flatten A Tensor**  \n",
    "\n",
    "A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.116992Z",
     "start_time": "2020-08-09T18:22:11.109918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since the argument t can be any tensor, we pass -1 as the second argument to the reshape() function. \n",
    "# In PyTorch, the -1 tells the reshape() function to figure out what the value should be based on the number of \n",
    "# elements contained within the tensor.\n",
    "\n",
    "def flatten(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.126200Z",
     "start_time": "2020-08-09T18:22:11.117760Z"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.ones(4, 3)\n",
    "print(t.shape, \"\\n\", flatten(t), \"\\n\", flatten(t).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenating Tensors:**  \n",
    "\n",
    "We combine tensors using the cat() function, and the resulting tensor will have a shape that depends on the shape of the two input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.133560Z",
     "start_time": "2020-08-09T18:22:11.127027Z"
    }
   },
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "t2 = torch.tensor([\n",
    "    [5,6],\n",
    "    [7,8]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.142878Z",
     "start_time": "2020-08-09T18:22:11.134314Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can combine t1 and t2 row-wise (axis-0) in the following way:\n",
    "torch.cat((t1, t2), dim=0), torch.cat((t1, t2), dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.151020Z",
     "start_time": "2020-08-09T18:22:11.143612Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can combine t1 and t2 column-wise (axis-1) in the following way:\n",
    "torch.cat((t1, t2), dim=1), torch.cat((t1, t2), dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical example for flattening:**   \n",
    "    Let’s look now at a hand written image of an eight from the MNIST dataset. This image has 2 distinct dimensions, height and width. The height and width are `18 x 18` respectively. These dimensions tell us that this is a `cropped image` because the MNIST dataset contains `28 x 28` images.  \n",
    "    Let’s see now how these two axes of height and width are flattened out into a single axis of length `324`.  \n",
    "    \n",
    "    \n",
    "<img src=\"assets/eight_13_13.PNG\" width=300px>  \n",
    "\n",
    "   The image below shows our flattened output with a single axis of length 324. The white on the edges corresponds to the white at the top and bottom of the image.\n",
    "\n",
    "In this example, we are flattening the entire tensor image, but what if we want to only flatten specific axes within the tensor? This is typically required when working with CNNs. Let’s see how we can flatten out specific axes of a tensor in code with PyTorch.  \n",
    "\n",
    "<img src=\"assets/eight_flatten.PNG\" width=900px>  \n",
    "\n",
    "Tensor inputs to a `convolutional neural network typically have 4 axes`, one for batch size, one for color channels, and one each for height and width.**(Batch Size, Channels, Height, Width)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.160064Z",
     "start_time": "2020-08-09T18:22:11.151763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Building A Tensor Representation For A Batch Of Gray Images\n",
    "\n",
    "t1 = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2]\n",
    "])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have three, rank-2 tensors. For our purposes here, we’ll consider these to be `three 4 x 4 images`. We will use them to create a batch that can be passed to a CNN.  \n",
    "    \n",
    "    Remember, batches are represented using a single tensor, so we’ll need to combine these three tensors into a single larger tensor that has three axes instead of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.169419Z",
     "start_time": "2020-08-09T18:22:11.162410Z"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.stack((t1, t2, t3))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `stack()` method to concatenate our sequence of three tensors along a new axis. Since we have three tensors along a new axis, we know the `length of this axis should be 3`, and indeed, we can see in the shape that we have 3 tensors that have height and width of 4.  \n",
    "\n",
    "The axis with a length of 3 represents the batch size while the axes of length 4 represent the height and width respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.177900Z",
     "start_time": "2020-08-09T18:22:11.170304Z"
    }
   },
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a rank-3 tensor that contains a batch of three 4 x 4 images. Now we need to convert this tensor into a form that a CNN expects i.e. an additional axis for the color channels.  \n",
    "\n",
    "We basically have an implicit single color channel for each of these image tensors, so in practice, these would be grayscale images.\n",
    "\n",
    "A CNN will expect to see an explicit color channel axis, so let’s add one by reshaping this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.186264Z",
     "start_time": "2020-08-09T18:22:11.178649Z"
    }
   },
   "outputs": [],
   "source": [
    "t = t.reshape(3,1,4,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have specified an axis of length 1 right after the batch size axis. Then, we follow with the height and width axes length 4. \n",
    "- Notice how the additional axis of length 1 doesn’t change the number of elements in the tensor. This is because the product of the components values doesn't change when we multiply by one.\n",
    "\n",
    "- The first axis has 3 elements. Each element of the first axis represents an image. For each image, we have a single color channel on the channel axis. Each of these channels contain 4 arrays that contain 4 numbers or scalar components.\n",
    "\n",
    "Let’s see this with code by indexing into this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.194278Z",
     "start_time": "2020-08-09T18:22:11.187048Z"
    }
   },
   "outputs": [],
   "source": [
    "# We have the first image\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.203817Z",
     "start_time": "2020-08-09T18:22:11.195065Z"
    }
   },
   "outputs": [],
   "source": [
    "# First channel in the first image\n",
    "t[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.212717Z",
     "start_time": "2020-08-09T18:22:11.204548Z"
    }
   },
   "outputs": [],
   "source": [
    "# First row of pixel in First channel of the first image\n",
    "t[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.221298Z",
     "start_time": "2020-08-09T18:22:11.213445Z"
    }
   },
   "outputs": [],
   "source": [
    "#  First pixel value in the first row of the first channel of the first image.\n",
    "t[0][0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flattening The Tensor Batch:** \n",
    "\n",
    "Let’s see how to flatten the images in this batch. Remember the whole batch is a single tensor that will be passed to the CNN, so we don’t want to flatten the whole thing. We only want to flatten the image tensors within the batch tensor.\n",
    "\n",
    "Let’s flatten the whole thing first just to see what it will look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.229428Z",
     "start_time": "2020-08-09T18:22:11.222036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function which we wrote above\n",
    "print(flatten(t), \"\\n\", flatten(t).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we should notice about this output is that we have flattened the entire batch, and this smashes all the images together into a single axis. \n",
    "    \n",
    "    Remember the ones represent the pixels from the first image, the twos the second image, and the threes from the third.  \n",
    "This flattened batch won’t work well inside our CNN because we need individual predictions for each image within our batch tensor, and now we have a flattened mess.\n",
    "\n",
    "So what is the solution now?  \n",
    "The solution here, is to flatten each image while still maintaining the batch axis. This means we want to flatten only part of the tensor. We want to flatten the, color channel axis with the height and width axes i.e. (C, H, W) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.237632Z",
     "start_time": "2020-08-09T18:22:11.230213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Here we are using pytorch built in function\n",
    "\n",
    "t.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.246130Z",
     "start_time": "2020-08-09T18:22:11.238355Z"
    }
   },
   "outputs": [],
   "source": [
    "t.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here, how we have specified the start_dim parameter. This tells the flatten() method which axis it should start the flatten operation. The one here is an index, so it’s the second axis which is the color channel axis. We skip over the batch axis, leaving it intact.\n",
    "\n",
    "Checking the shape, we can see that we have a rank-2 tensor with three single color channel images that have been flattened out into 16 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flattening An RGB Image:**  \n",
    "If we flatten an RGB image, what happens to the color ?\n",
    "\n",
    "> Each color channel will be flattened first. Then, the flattened channels will be lined up side by side on a single axis of the tensor. Let's look at an example in code.\n",
    "\n",
    "We'll build an example RGB image tensor with a height of two and a width of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.253961Z",
     "start_time": "2020-08-09T18:22:11.246888Z"
    }
   },
   "outputs": [],
   "source": [
    "r = torch.ones(1,2,2)\n",
    "g = torch.ones(1,2,2) + 1\n",
    "b = torch.ones(1,2,2) + 2\n",
    "\n",
    "img = torch.cat(\n",
    "    (r,g,b),\n",
    "    dim=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.262395Z",
     "start_time": "2020-08-09T18:22:11.254725Z"
    }
   },
   "outputs": [],
   "source": [
    "# This should be our desired tensor. We can verify this by checking the shape like so:\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.270967Z",
     "start_time": "2020-08-09T18:22:11.263188Z"
    }
   },
   "outputs": [],
   "source": [
    "# We have three color channels with a height and width of two. We can also inspect this tensor's data like so:\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.279317Z",
     "start_time": "2020-08-09T18:22:11.271695Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can see how this will look by flattening the image tensor.\n",
    "# In this case, we are flattening the whole image.\n",
    "\n",
    "img.flatten(start_dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T18:22:11.287576Z",
     "start_time": "2020-08-09T18:22:11.280091Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can also flatten only the channels:\n",
    "\n",
    "img.flatten(start_dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245.76px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
